{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04745c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi as wiki\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import pymorphy2\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36bc5955",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wiki = wiki.Wikipedia('ru')\n",
    "page_py_1 = wiki_wiki.page('Категория:Кухни народов мира')\n",
    "page_py_2 = wiki_wiki.page('Категория:Кухни по странам')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8b94630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorymembers(categorymembers, level=0, max_level=0):\n",
    "        return categorymembers.values()\n",
    "\n",
    "cuisines_1 = get_categorymembers(page_py_1.categorymembers)\n",
    "cuisines_2 = get_categorymembers(page_py_2.categorymembers)\n",
    "\n",
    "cuisines_list = []\n",
    "for page in cuisines_1:\n",
    "    if 'Категория:' in page.title:\n",
    "        cuisines_list.append(page.title)\n",
    "\n",
    "for page in cuisines_2:\n",
    "    if 'Категория:' in page.title:\n",
    "        cuisines_list.append(page.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb4567b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisines_list = list(set(cuisines_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a8776f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Категория:Татарская кухня',\n",
       " 'Категория:Тунисская кухня',\n",
       " 'Категория:Английская кухня',\n",
       " 'Категория:Аргентинская кухня',\n",
       " 'Категория:Хорватская кухня']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuisines_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dae3fd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7374d723",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [17:38<00:00,  7.10s/it]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "f = open('cuisines.txt', 'w')\n",
    "\n",
    "names_of_cuis = []\n",
    "\n",
    "for cuisine in tqdm(cuisines_list):\n",
    "    dishes = get_categorymembers(wiki_wiki.page(cuisine).categorymembers)\n",
    "    for dish in dishes:\n",
    "        names_of_cuis.append(cuisine)\n",
    "        summary = re.sub('^.*? — ', '', dish.summary)\n",
    "        punct = '[!\"#$%&()*+,./:;<=>?@[\\]^_`{|}~„“«»†*/\\—–‘’]'\n",
    "        nums = '[0-9]'\n",
    "        summary = re.sub('\\n', ' ', summary)\n",
    "        summary = re.sub(punct, '', summary)\n",
    "        summary = re.sub(nums, '', summary)\n",
    "        summary = re.sub(r'(?![А-яё ]).', '', summary)\n",
    "        try:\n",
    "            f.write(summary + \" \")\n",
    "        except Exception:\n",
    "            pass  \n",
    "        f.write('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d7081537",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'labels.txt', 'w') as fp:\n",
    "    for label in names_of_cuis:\n",
    "        fp.write(label + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ad3dc2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Кулинарное искусство татарского народа богато своими национальными и культурными традициями уходящим'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('cuisines.txt', 'r', encoding='cp1251') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "texts = text.split('\\n\\n\\n')\n",
    "texts[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2aab195f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yaroslav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c94b1d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = nltk.WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6095bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [str(x) for x in np.arange(1900, 2022)]\n",
    "def process_data(data):\n",
    "    texts = []\n",
    "    targets = []\n",
    "    \n",
    "    # поочередно проходим по всем новостям в списке\n",
    "    for item in data:   \n",
    "        tokens = word_tokenizer.tokenize(item)\n",
    "        tokens = [word for word in tokens if (word not in string.punctuation and word not in stop_words and word not in dates)]\n",
    "        texts.append(tokens) # добавляем в предобработанный список\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7052fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = process_data(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23eb565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "tokenized_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d47e55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yaroslav\\AppData\\Local\\Temp\\ipykernel_11272\\135266771.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(range(len(texts))):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e453b9e6ab5c4197aca23d62aa82978a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3633 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm_notebook(range(len(texts))):\n",
    "    text_lemmatized = [morph.parse(x)[0].normal_form for x in texts[i]]\n",
    "    tokenized_texts.append(text_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "05575fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['кулинарный',\n",
       " 'искусство',\n",
       " 'татарский',\n",
       " 'народ',\n",
       " 'богато',\n",
       " 'свой',\n",
       " 'национальный',\n",
       " 'культурный',\n",
       " 'традиция',\n",
       " 'уходить']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "85965040",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rus_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    sw = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9ab86e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, text in enumerate(tokenized_texts):\n",
    "    text = [w for w in text if w not in sw]\n",
    "    tokenized_texts[ind] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e75531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'text_lemmatized.txt', 'w') as fp:\n",
    "    for item in tokenized_texts:\n",
    "        fp.write(\"%s\\n\\n\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0d9b459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\yaroslav\\.conda\\envs\\my-ml-env\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\yaroslav\\.conda\\envs\\my-ml-env\\lib\\site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\yaroslav\\.conda\\envs\\my-ml-env\\lib\\site-packages (from gensim) (6.0.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\yaroslav\\.conda\\envs\\my-ml-env\\lib\\site-packages (from gensim) (1.22.3)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\yaroslav\\.conda\\envs\\my-ml-env\\lib\\site-packages (from gensim) (0.29.28)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e1d4f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import *\n",
    "from gensim import corpora\n",
    "from gensim import similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b82d81e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dictionary...\n",
      "Original: Dictionary<15918 unique tokens: ['богато', 'век', 'глубь', 'день', 'жизнь']...>\n",
      "Filtered: Dictionary<4540 unique tokens: ['век', 'день', 'жизнь', 'искусство', 'история']...>\n",
      "Vectorizing corpus...\n"
     ]
    }
   ],
   "source": [
    "print('Making dictionary...')\n",
    "dictionary = corpora.Dictionary(tokenized_texts)\n",
    "print('Original: {}'.format(dictionary))\n",
    "dictionary.filter_extremes(no_below = 5, no_above = 0.9, keep_n=None)\n",
    "dictionary.save('polkrug.dict')\n",
    "print('Filtered: {}'.format(dictionary))\n",
    "\n",
    "print('Vectorizing corpus...')\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
    "corpora.MmCorpus.serialize('polkrug.model', corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "295290da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3633, 3633)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_texts), len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c5253bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 2),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 2),\n",
       " (10, 2),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 1),\n",
       " (16, 1),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 1),\n",
       " (24, 1),\n",
       " (25, 1)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7478bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
